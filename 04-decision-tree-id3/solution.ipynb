{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Zadanie 4 - Drzewo decyzyjne ID3**\n",
    "\n",
    "Cel zadania polega na implementacji drzewa decyzyjnego tworzonego algorytmem ID3 z ograniczeniem maksymalnej głębokości drzewa, jak również na stworzeniu i zbadaniu jakości klasyfikatora dla zbioru danych [Tic-Tac-Toe Endgame](https://archive.ics.uci.edu/dataset/101/tic+tac+toe+endgame).\n",
    "\n",
    "**Kroki do wykonania:**\n",
    "- Zaimplementuj drzewo decyzyjne ID3 (z ograniczeniem jego maksymalnej głębokości).\n",
    "- Zbadaj skuteczność działania kasyfikatora dla zbioru danych Tic-Tac-Toe Endgame, obliczając dokładność i macierz pomyłek.\n",
    "\n",
    "**Uwagi**\n",
    "- Należy pamiętać o podziale danych na zbiory trenujący, walidacyjny i testowy.\n",
    "- Zaimplementowana metoda powinna być uniwersalna - nie należy \"zaszywać\" na sztywno w kodzie np. nazwy pliku ze zbiorem danych czy wartości atrybutów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset\n",
    "tic_tac_toe_endgame = fetch_ucirepo(id=101)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "X = tic_tac_toe_endgame.data.features\n",
    "y = tic_tac_toe_endgame.data.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 101, 'name': 'Tic-Tac-Toe Endgame', 'repository_url': 'https://archive.ics.uci.edu/dataset/101/tic+tac+toe+endgame', 'data_url': 'https://archive.ics.uci.edu/static/public/101/data.csv', 'abstract': 'Binary classification task on possible configurations of tic-tac-toe game', 'area': 'Games', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 958, 'num_features': 9, 'feature_types': ['Categorical'], 'demographics': [], 'target_col': ['class'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 1991, 'last_updated': 'Mon Aug 19 1991', 'dataset_doi': '10.24432/C5688J', 'creators': ['David Aha'], 'intro_paper': None, 'additional_info': {'summary': 'This database encodes the complete set of possible board configurations at the end of tic-tac-toe games, where \"x\" is assumed to have played first.  The target concept is \"win for x\" (i.e., true when \"x\" has one of 8 possible ways to create a \"three-in-a-row\").  \\r\\n\\r\\nInterestingly, this raw database gives a stripped-down decision tree algorithm (e.g., ID3) fits.  However, the rule-based CN2 algorithm, the simple IB1 instance-based learning algorithm, and the CITRE feature-constructing decision tree algorithm perform well on it.', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': '    1. top-left-square: {x,o,b}\\r\\n    2. top-middle-square: {x,o,b}\\r\\n    3. top-right-square: {x,o,b}\\r\\n    4. middle-left-square: {x,o,b}\\r\\n    5. middle-middle-square: {x,o,b}\\r\\n    6. middle-right-square: {x,o,b}\\r\\n    7. bottom-left-square: {x,o,b}\\r\\n    8. bottom-middle-square: {x,o,b}\\r\\n    9. bottom-right-square: {x,o,b}\\r\\n   10. Class: {positive,negative}', 'citation': None}}\n"
     ]
    }
   ],
   "source": [
    "# metadata\n",
    "print(tic_tac_toe_endgame.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   name     role         type demographic description units  \\\n",
      "0                 class   Target  Categorical        None        None  None   \n",
      "1       top-left-square  Feature  Categorical        None        None  None   \n",
      "2     top-middle-square  Feature  Categorical        None        None  None   \n",
      "3      top-right-square  Feature  Categorical        None        None  None   \n",
      "4    middle-left-square  Feature  Categorical        None        None  None   \n",
      "5  middle-middle-square  Feature  Categorical        None        None  None   \n",
      "6   middle-right-square  Feature  Categorical        None        None  None   \n",
      "7    bottom-left-square  Feature  Categorical        None        None  None   \n",
      "8  bottom-middle-square  Feature  Categorical        None        None  None   \n",
      "9   bottom-right-square  Feature  Categorical        None        None  None   \n",
      "\n",
      "  missing_values  \n",
      "0             no  \n",
      "1             no  \n",
      "2             no  \n",
      "3             no  \n",
      "4             no  \n",
      "5             no  \n",
      "6             no  \n",
      "7             no  \n",
      "8             no  \n",
      "9             no  \n"
     ]
    }
   ],
   "source": [
    "# variable information\n",
    "print(tic_tac_toe_endgame.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.cgildren = []\n",
    "\n",
    "    def add_child(self, child):\n",
    "        self.children.append(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeID3:\n",
    "    def __init__(self, X, y, train_val_test_split = [0.7, 0.15, 0.15], max_depth = 10, min_samples_split = 2):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.train_val_test_split = train_val_test_split\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "\n",
    "    def get_train_val_test_split(self):\n",
    "        # train is now train_val_test_split[0] of the entire data set\n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size = 1 - self.train_val_test_split[0], random_state = 42)\n",
    "        # split the test set into validation and test sets\n",
    "        X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size = self.train_val_test_split[2]/\n",
    "                                                        (self.train_val_test_split[1] + self.train_val_test_split[2]), random_state = 42)\n",
    "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "    def calculate_dataset_entropy(self, y):\n",
    "        entropy = 0\n",
    "        for class_name in np.unique(y):\n",
    "            p = np.sum(y == class_name) / len(y)\n",
    "            entropy += -p * np.log2(p)\n",
    "        return entropy\n",
    "\n",
    "    def calculate_dataset_divide_entropy(self, feature_name, X, y):\n",
    "        entropy = 0\n",
    "        for value in np.unique(X[feature_name]):\n",
    "            entropy += np.sum(X[feature_name] == value) / len(X) * self.calculate_dataset_entropy(y[X[feature_name] == value])\n",
    "\n",
    "        return entropy\n",
    "\n",
    "    def information_gain(self, X, y, feature):\n",
    "        return self.calculate_dataset_entropy(y) - self.calculate_dataset_divide_entropy(feature, X, y)\n",
    "\n",
    "    def choose_best_feature(self, X, y):\n",
    "        best_feature = None\n",
    "        best_information_gain = 0\n",
    "\n",
    "        for feature in X.columns:\n",
    "            information_gain = self.information_gain(X, y, feature)\n",
    "            print(f'Feature: {feature}, Information Gain: {information_gain.item()}')\n",
    "            if information_gain.item() > best_information_gain:\n",
    "                best_information_gain = information_gain\n",
    "                best_feature = feature\n",
    "\n",
    "        return best_feature\n",
    "\n",
    "    def fit(self):\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test = self.get_train_val_test_split()\n",
    "\n",
    "        self.root = Node(data = X_train)\n",
    "        self.build_tree(self.root, X_train, y_train, 0)\n",
    "\n",
    "    def build_tree(self, node, X, y, depth):\n",
    "        if depth == self.max_depth:\n",
    "            return None\n",
    "\n",
    "        # get split feature\n",
    "        split_feature = self.choose_best_feature(X, y)\n",
    "        if split_feature is None:\n",
    "            return None\n",
    "\n",
    "        # split the data\n",
    "        for value in np.unique(X[split_feature]):\n",
    "            child_node = Node(data = X[X[split_feature] == value])\n",
    "            node.add_child(child_node)\n",
    "            self.build_tree(child_node, X[X[split_feature] == value], y[X[split_feature] == value], depth + 1)\n",
    "\n",
    "    def predict(self, X):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "id3_tree = DecisionTreeID3(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id3_tree.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
