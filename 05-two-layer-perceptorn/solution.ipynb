{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 5 - Two-Layer Perceptron**\n",
    "\n",
    "Implementing a two-layer perceptron and training it to represent a given function $f(x)$, describing the Laplace distribution. This function is given by the formula:\n",
    "\n",
    "\\begin{equation}\n",
    "    f(x) = \\frac{1}{2b}e^{-\\frac{|x-u|}{b}}\n",
    "\\end{equation}\n",
    "gdzie:\n",
    "- Range $x: [-8, 8]$\n",
    "- Values of $\\mu$ and $b$: $\\mu = 0$ i $b = 1$\n",
    "\n",
    "**Steps:**\n",
    "1. Implement a two-layer perceptron to represent the function $f(x)$ for a given range of $x$  and the value of $\\mu$ and $b$.\n",
    "2. Check the quality of the approximation by calculating the Mean Squared Error (MSE) and Mean Absolute Error (MAE) between the actual values of the function and the values predicted by the network.\n",
    "3. Create the graph of the real function and the function predicted by the network.\n",
    "4. Investigate how the number of neurons in the hidden layer affects the quality of the approximation by changing its value and comparing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace(x, mu=0, b=1):\n",
    "    return 1/(2*b) * np.exp(-np.abs(x-mu)/b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_function(x, y, title, xlabel='x', ylabel='f(x)'):\n",
    "    plt.plot(x, y)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10, 10, 100)\n",
    "y = laplace(x)\n",
    "\n",
    "visualize_function(x, y, 'Laplace Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10, 10, 100)\n",
    "y = sigmoid(x)\n",
    "\n",
    "visualize_function(x, y, 'Sigmoid Function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y_i$ - true value, $\\hat{y}_i$ - predicted value, $n$ - number of samples\n",
    "\n",
    "**Mean Squared Error (MSE)**:\n",
    "\n",
    "$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "**Mean Absolute Error (MAE)**:\n",
    "\n",
    "$\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dla jednego przykladu n=1\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return (y_true - y_pred) ** 2\n",
    "\n",
    "def mae_loss(y_true, y_pred):\n",
    "    return np.abs(y_true - y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss_derivative(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true)\n",
    "\n",
    "def mae_loss_derivative(y_true, y_pred):\n",
    "    return np.where(y_pred > y_true, 1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two-layer perceptron consists of three layers:\n",
    "1. input layer - contains input features $x$.\n",
    "2. hidden layer - contains neurons that process inputs and pass them to the output layer.\n",
    "3. output layer - contains neurons that process data from the hidden layer and generate a result.\n",
    "\n",
    "**Initialization of network parameters:**.\n",
    "- The weights of the output neurons are zeroed.\n",
    "- The weights of the hidden layer neurons are initialized randomly with a distribution $~U(-1/\\sqrt{we}, 1/\\sqrt{we})$, where $we$ is the number of inputs to the neuron.\n",
    "\n",
    "**Forward propagation:**\n",
    "\\begin{equation}\n",
    "    h = \\sigma(W_1 \\cdot x + b_1)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{y} = W_2 \\cdot h + b_2\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "gdzie:\n",
    "- $W_1$ - input layer weights\n",
    "- $b_1$ - input layer bias\n",
    "- $\\sigma$ - hidden layer activation function\n",
    "- $W_2$ - output layer weights\n",
    "- $b_2$ - output layer bias\n",
    "\n",
    "\n",
    "**Backward propagation:**\n",
    "The weights are updated based on the error made by the network. The error is calculated based on the difference between the network's actual and predicted values (MSE or MAE). The weights are then updated in the direction opposite to the gradient of the cost function. Subsequent weights are updated according to the rule:\n",
    "\n",
    "\\begin{equation}\n",
    "    W = W - \\alpha \\frac{\\partial L}{\\partial W}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    b = b - \\alpha \\frac{\\partial L}{\\partial b}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "where:\n",
    "- $L$ - cost function\n",
    "- $\\alpha$ - learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, size_in, size_hidden, size_out):\n",
    "        self.W1, self.W2, self.b1, self.b2 = self.initialize_parameters(size_in, size_hidden, size_out)\n",
    "\n",
    "    def initialize_parameters(self, size_in, size_hidden, size_out):\n",
    "        # initialize weights using Uniform distribution\n",
    "        uniform_range = 1/np.sqrt(size_in)\n",
    "        W1 = np.random.uniform(-uniform_range, uniform_range, (size_hidden, size_in))\n",
    "        W2 = np.random.uniform(-uniform_range, uniform_range, (size_out, size_hidden))\n",
    "        b1 = np.zeros((size_hidden, 1))\n",
    "        b2 = np.zeros((size_out, 1))\n",
    "        return W1, W2, b1, b2\n",
    "\n",
    "    def forward(self, x):\n",
    "        z1 = np.dot(self.W1, x) + self.b1\n",
    "        a1 = sigmoid(z1)\n",
    "        z2 = np.dot(self.W2, a1) + self.b2\n",
    "        return z1, a1, z2\n",
    "\n",
    "    def backpropagation(self, x, y, z1, a1, z2):\n",
    "        dL_dz2 = mse_loss_derivative(y, z2)\n",
    "\n",
    "        dL_dW2 = np.dot(dL_dz2, a1.T)\n",
    "        dL_db2 = dL_dz2\n",
    "\n",
    "        dL_da1 = np.dot(self.W2.T, dL_dz2)\n",
    "        dL_dz1 = dL_da1 * sigmoid_derivative(z1)\n",
    "        dL_dW1 = np.dot(dL_dz1, x.T)\n",
    "        dL_db1 = dL_dz1\n",
    "\n",
    "        return dL_dW1, dL_db1, dL_dW2, dL_db2\n",
    "\n",
    "    def update_parameters(self, dW1, db1, dW2, db2, learning_rate):\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "\n",
    "    def shuffle(self, x, y):\n",
    "        indices = np.arange(x.shape[1])\n",
    "        np.random.shuffle(indices)\n",
    "        return x[:, indices], y[:, indices]\n",
    "\n",
    "    def train(self, x, y, epochs, learning_rate):\n",
    "        for i in range(epochs):\n",
    "            epoch_loss = 0\n",
    "\n",
    "            # shuffle samples\n",
    "            x, y = self.shuffle(x, y)\n",
    "\n",
    "            for j in range(x.shape[1]):\n",
    "                x_sample = x[:, j].reshape(-1, 1)\n",
    "                y_sample = y[:, j].reshape(-1, 1)\n",
    "                z1, a1, z2 = self.forward(x_sample)\n",
    "                dW1, db1, dW2, db2 = self.backpropagation(x_sample, y_sample, z1, a1, z2)\n",
    "                self.update_parameters(dW1, db1, dW2, db2, learning_rate)\n",
    "\n",
    "                epoch_loss += mse_loss(y_sample, z2)\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(f'Epoch {i}, loss: {epoch_loss / x.shape[1]}')\n",
    "\n",
    "    def predict(self, x):\n",
    "        _, _, z2 = self.forward(x)\n",
    "        return z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 0\n",
    "b = 1\n",
    "size_in = 1\n",
    "size_hidden = 748\n",
    "size_out = 1\n",
    "\n",
    "# generate data\n",
    "x_all = np.linspace(-8, 8, 10_000).reshape(1, -1)\n",
    "y_all = laplace(x_all, mu, b)\n",
    "\n",
    "# shuffle data\n",
    "indices = np.arange(x_all.shape[1])\n",
    "np.random.shuffle(indices)\n",
    "x_all_shuffled = x_all[:, indices]\n",
    "y_all_shuffled = y_all[:, indices]\n",
    "\n",
    "# split to train and test set\n",
    "x_train = x_all_shuffled[:, :800]\n",
    "y_train = y_all_shuffled[:, :800]\n",
    "x_test = x_all_shuffled[:, 800:]\n",
    "y_test = y_all_shuffled[:, 800:]\n",
    "\n",
    "x = x_train.reshape(1, -1)\n",
    "y = y_train.reshape(1, -1)\n",
    "\n",
    "model = Perceptron(size_in, size_hidden, size_out)\n",
    "model.train(x, y, epochs=100, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(x, y_true, y_pred, title, xlabel='x', ylabel='f(x)'):\n",
    "    plt.plot(x, y_true, label='True')\n",
    "    plt.plot(x, y_pred, label='Predicted')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = []\n",
    "\n",
    "for x_sample, y_sample in zip(x_test[0], y_test[0]):\n",
    "    x_sample = np.array([x_sample]).reshape(1, 1)\n",
    "    y_preds.append(model.predict(x_sample)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indices = np.argsort(x_test[0])\n",
    "x_test_sorted = x_test[0][sorted_indices]\n",
    "y_test_sorted = y_test[0][sorted_indices]\n",
    "y_test_preds = np.array(y_preds)[sorted_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(x_test_sorted, y_test_sorted, y_test_preds, 'Predictions on Test Set')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
