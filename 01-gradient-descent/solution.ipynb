{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 1 - Gradient Descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement gradient descent algorithm and test it for functions:\n",
    "- $f(x) = x_1^2 + x_2^2$\n",
    "- [Matyas function](https://www.sfu.ca/~ssurjano/matya.html) (for 2 dimensions)\n",
    "\n",
    "**Steps:**\n",
    "1. Implement the gradient descent algorithm.\n",
    "2. Investigate the effect of the value of the step parameter on the convergence of the method - plot the pairs (value of the objective function, number of iterations).\n",
    "3. For a fixed value of the step parameter, investigate the behaviour of the algorithm for three selected starting points. Present the results in visual form.\n",
    "\n",
    "**Notes:**\n",
    "- Stop conditions: maximum number of iterations, gradient convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import List, Callable, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define constants\n",
    "MIN_X = -10\n",
    "MAX_X = 10\n",
    "PLOT_STEP = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 20_000\n",
    "epsilon = 1e-6\n",
    "learning_rate = 0.1\n",
    "learning_rate_list = [0.1, 0.01, 0.001, 0.0001]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Definition of the objective function and its gradient**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x: np.ndarray) -> float:\n",
    "    return np.sum(x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_dfdx(x: np.ndarray) -> np.ndarray:\n",
    "    return 2 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matyas(x: np.ndarray) -> float:\n",
    "    return 0.26 * (x[0]**2 + x[1]**2) - 0.48 * x[0] * x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matyas_dfdx(x: np.ndarray) -> np.ndarray:\n",
    "    return np.array([0.52*x[0] - 0.48*x[1], 0.52*x[1] - 0.48*x[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Visualisation of the objective function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meshgrid_input(min_x: float=MIN_X, max_x: float=MAX_X, step: int=PLOT_STEP) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create a meshgrid for the input space.\n",
    "\n",
    "    Parameters:\n",
    "        min_x (float): Minimum value for the x-axis.\n",
    "        max_x (float): Maximum value for the x-axis.\n",
    "        step (int): Number of steps for the meshgrid.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The meshgrid for the input space.\n",
    "    \"\"\"\n",
    "    x1 = np.linspace(min_x, max_x, step)\n",
    "    x2 = np.linspace(min_x, max_x, step)\n",
    "    X1, X2 = np.meshgrid(x1, x2)\n",
    "    return X1, X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_func_3d(obj_fun: Callable[[np.ndarray], float], trajectory: np.ndarray = None) -> None:\n",
    "    \"\"\"\n",
    "    Visualize the objective function and the gradient descent trajectory in 3D.\n",
    "\n",
    "    Parameters:\n",
    "        obj_fun (Callable[[np.ndarray], float]): The objective function.\n",
    "        trajectory (np.ndarray): The trajectory of the gradient descent.\n",
    "    \"\"\"\n",
    "    # calculate output for the objective function\n",
    "    X1, X2 = get_meshgrid_input()\n",
    "    Z = np.apply_along_axis(obj_fun, 0, np.array([X1, X2]))\n",
    "\n",
    "    # plot the 3D surface\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.plot_surface(X1, X2, Z, cmap='viridis', alpha=0.8)\n",
    "\n",
    "    # set labels and title\n",
    "    ax.set_xlabel('x1', labelpad=10)\n",
    "    ax.set_ylabel('x2', labelpad=10)\n",
    "    ax.set_zlabel('Objective Function Value', labelpad=10)\n",
    "    ax.set_title('Objective Function Visualization')\n",
    "\n",
    "\n",
    "    # plot the trajectory\n",
    "    if trajectory is not None:\n",
    "        # get the minimum point\n",
    "        min_point = trajectory[-1]\n",
    "        min_x, min_y = min_point[0], min_point[1]\n",
    "\n",
    "        # plot the minimum point and the trajectory\n",
    "        ax.scatter(min_x, min_y, func(np.array([min_x, min_y])), color='yellow', label='Minimum found by gradient descent alg.')\n",
    "        ax.plot(trajectory[:, 0], trajectory[:, 1], np.array(Z), marker='o', color='red', label='Gradient Descent Steps', alpha=0.5)\n",
    "\n",
    "        plt.legend()\n",
    "\n",
    "    ax.set_box_aspect(aspect=None, zoom=0.8)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_func_3d(matyas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Implementation of the gradient descent algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(\n",
    "    grad_dfdx: Callable[[np.ndarray], np.ndarray],\n",
    "    func: Callable[[np.ndarray], float],\n",
    "    x: np.ndarray,\n",
    "    learning_rate: float,\n",
    "    max_iter: int,\n",
    "    gradient_tolerance: float,\n",
    ") -> Tuple[List[np.ndarray], List[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Calculate the gradient descent trajectory for a function.\n",
    "\n",
    "    Parameters:\n",
    "        grad_dfdx (Callable[[np.ndarray], np.ndarray]): Calculates the gradient of the function at a point.\n",
    "        x (np.ndarray): The initial point.\n",
    "        learning_rate (float): Step size for the gradient descent.\n",
    "        max_iter (int): Maximum number of iterations.\n",
    "        gradient_tolerance (float): Convergence criterion for the gradient.\n",
    "\n",
    "    Returns:\n",
    "        List[np.ndarray]: The trajectory of the gradient descent.\n",
    "        List[np.ndarray]: The function values.\n",
    "    \"\"\"\n",
    "    trajectory = [x.copy()]\n",
    "    func_val = [func(x)]\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        dfdx = grad_dfdx(x)\n",
    "        if np.linalg.norm(dfdx) < gradient_tolerance:\n",
    "            print(f'Gradient descent converged after {len(trajectory)} iterations.')\n",
    "            break\n",
    "\n",
    "        x -= learning_rate * dfdx\n",
    "        trajectory.append(x.copy())\n",
    "        func_val.append(func(x))\n",
    "\n",
    "    return trajectory, func_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory, func_val = gradient_descent(matyas_dfdx, matyas, np.array([3.0, 3.0]), learning_rate, max_iter, epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Visualisation of the convergence of the algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_fun(obj_fun: Callable[[np.ndarray], float], trajectory: np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    Visualize the objective function and the gradient descent trajectory.\n",
    "\n",
    "    Parameters:\n",
    "        obj_fun (Callable[[np.ndarray], float]): The objective function.\n",
    "        trajectory (np.ndarray): The trajectory of the gradient descent.\n",
    "    \"\"\"\n",
    "    min_point = trajectory[-1]\n",
    "    min_x, min_y = min_point[0], min_point[1]\n",
    "\n",
    "    print(f\"Minimum found at x = {np.round(min_x, 2)}, y = {np.round(min_y, 2)}, f(x, y) = {np.round(obj_fun(min_point), 2)}\")\n",
    "    print(f\"Number of steps: {len(trajectory) - 1}\")\n",
    "\n",
    "    X1, X2 = get_meshgrid_input()\n",
    "    Z = np.apply_along_axis(obj_fun, 0, np.array([X1, X2]))\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.pcolormesh(X1, X2, Z, cmap='viridis', shading='auto')\n",
    "    plt.colorbar(label='Value of the objective function')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    # plt.title('Wizualizacja funkcji celu')\n",
    "\n",
    "    plt.scatter(min_x, min_y, color='yellow', label='The minimum found by the algorithm.')\n",
    "    plt.plot(trajectory[:, 0], trajectory[:, 1], marker='.', color='red', label='Steps of the gradient descent algorithm', alpha=0.5)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_trajectory, func_val = gradient_descent(\n",
    "    matyas_dfdx,\n",
    "    matyas,\n",
    "    np.array([5.0, 9.0]),\n",
    "    0.001,\n",
    "    10_000,\n",
    "    1e-6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_fun(matyas, np.array(func_trajectory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Visualisation of the change in the value of the objective function as a function of the number of iterations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_results = []\n",
    "\n",
    "for learning_rate in learning_rate_list:\n",
    "    trajectory, func_val = gradient_descent(\n",
    "        func_dfdx,\n",
    "        func,\n",
    "        np.array([-1.0, 9.0]),\n",
    "        learning_rate,\n",
    "        max_iter,\n",
    "        1e-6,\n",
    "    )\n",
    "    function_results.append(func_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_value_change(func_val: List[List[float]], learning_rates: List[float]) -> None:\n",
    "    \"\"\"\n",
    "    Visualize the change in the objective function value over iterations.\n",
    "\n",
    "    Parameters:\n",
    "        func_val (List[float]): The objective function value over iterations.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i in range(len(func_val)):\n",
    "        plt.plot(func_val[i], marker='.', label=f'{learning_rates[i]}', alpha=0.5)\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('Iteration number')\n",
    "    plt.ylabel('Value of the objective function')\n",
    "    plt.title('Change in the value of the objective function depending on the learning step')\n",
    "\n",
    "    legend = plt.legend()\n",
    "    legend.set_title('Krok uczenia')\n",
    "\n",
    "    legend.set_bbox_to_anchor((1, 1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_value_change(function_results, learning_rate_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
