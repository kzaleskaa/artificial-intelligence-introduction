{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 6 - Q-learning**\n",
    "Implementation of the Q-learning algorithm and applying it to solve the problem [Cliff Walking](https://gymnasium.farama.org/environments/toy_text/cliff_walking/). This environment is available in the gymnasium package `(gym.make('CliffWalking-v0')`.\n",
    "\n",
    "**Steps:**\n",
    "1. Q-learning algorithm implementation.\n",
    "2. Investigate the performance of the algorithm for the Cliff Walking problem for different values of the learning rate and different numbers of episodes (in the process of training).\n",
    "\n",
    "**Notes:**\n",
    "- The implementation of the algorithm should be usable for a variety of environments with a discrete space of states and actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from IPython.display import  clear_output\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CliffWalking-v0\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Q-learning algorithm implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Action selection by epsilon greedy method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(Q: np.ndarray, state: int, epsilon: float) -> int:\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        max_value = np.max(Q[state])\n",
    "        max_keys = np.where(Q[state] == max_value)[0]\n",
    "        return np.random.choice(max_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation of the Q-learning algorithm training process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_learning(env, beta=0.1, gamma=0.99, epsilon=0.1, max_episodes=5_000, max_steps=1000):\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "    training_rewards = []\n",
    "\n",
    "    for _ in range(max_episodes):\n",
    "        state, _ = env.reset()\n",
    "\n",
    "        steps_counter = 0\n",
    "\n",
    "        reward_per_episode = 0\n",
    "\n",
    "        while steps_counter < max_steps:\n",
    "            action = choose_action(Q, state, epsilon)\n",
    "            next_state, reward, done, _,_ = env.step(action)\n",
    "\n",
    "            Q[state, action] += beta * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            steps_counter += 1\n",
    "            reward_per_episode += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        training_rewards.append(reward_per_episode)\n",
    "\n",
    "    return Q, training_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation of the Q-learning algorithm testing process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_q_learning(env: gym.Env, Q: np.ndarray, max_steps=1000):\n",
    "    print(\"Testing Q-learning\")\n",
    "    state, _ = env.reset()\n",
    "    env.render()\n",
    "    total_reward = 0\n",
    "    step_counter = 0\n",
    "    while step_counter < max_steps:\n",
    "        max_values = np.max(Q[state])\n",
    "        max_keys = np.where(Q[state] == max_values)[0]\n",
    "        action = np.random.choice(max_keys)\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        env.render()\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "        step_counter += 1\n",
    "    print(\"Total reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Visualize the results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization of award results in subsequent episodes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_rewards_per_episode(rewards_per_episode, beta, episodes_num):\n",
    "    plt.plot(rewards_per_episode)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Award\")\n",
    "    plt.title(f\"Dependence of reward on episode for beta={beta}, number of episodes={episodes_num}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization of the environment and the agent's strategy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_steps(env: gym.Env, Q: np.ndarray):\n",
    "    env.reset()\n",
    "    env.render()\n",
    "    total_reward = 0\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    while True:\n",
    "        max_values = np.max(Q[state])\n",
    "        max_keys = np.where(Q[state] == max_values)[0]\n",
    "        action = np.random.choice(max_keys)\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        frames.append({\n",
    "            \"frame\": env.render(),\n",
    "            \"state\": state,\n",
    "            \"action\": action,\n",
    "            \"reward\": reward\n",
    "        })\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_frames(frames: List[dict]):\n",
    "    for _, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame[\"state\"])\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        plt.imshow(frame[\"frame\"])\n",
    "        plt.show()\n",
    "        sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Experiments**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The value of the reward in subsequent episodes for the random strategy (epsilon=1)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_random, rewards_random = train_q_learning(env, epsilon=0.1, max_episodes=2_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_rewards_per_episode(rewards_random, beta=0.1, episodes_num=2_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reward value in subsequent episodes for epsilon greedy strategy (epsilon=0.1) for different values of learning rate and number of episodes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BETA_VALUES = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "EPISODES_VALUES = [500, 1000, 5000, 10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for beta in BETA_VALUES:\n",
    "    for episodes in EPISODES_VALUES:\n",
    "        res = []\n",
    "        for _ in range(5):\n",
    "            res.append(train_q_learning(env, beta=beta, max_episodes=episodes)[1])\n",
    "        # visualize mean rewards per episode\n",
    "        visualize_rewards_per_episode(np.mean(res, axis=0), beta, episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example visualization of the environment and agent strategy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, rewards = train_q_learning(env, beta=0.1, max_episodes=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = get_all_steps(env, Q)\n",
    "print_frames(frames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
